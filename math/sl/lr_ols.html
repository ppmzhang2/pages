<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Linear Regression: Ordinary Least Squares &#8212; Machine Learning Notes</title>
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/basic_mod.css?v=0.7.0-1" />
    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/sphinx_highlight.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../_static/js/disqus.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <script src="../../_static/js/petite-vue.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Linear Regression: Formula Derivation" href="lr_formula.html" />
    <link rel="prev" title="Quadratic Discriminant Analysis" href="qda.html" /> 
  </head><body data-dark_mode_code_blocks="true">

<div id="top_nav">
    

    <nav>
        
            
        

        <p id="toggle_sidebar">
            <a href="#" title="Toggle sidebar">|||</a>
        </p>
        <h1><a href="../../index.html" title="Go to homepage">ML Notes</a></h1>
            <a id="source_link" href="https://github.com/ppmzhang2/ppmzhang2.github.io">
    
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512">
            <path fill="white" d="M 244.8,8 C 106.1,8 0,113.3 0,252 c 0,110.9 69.8,205.8 169.5,239.2 12.8,2.3 17.3,-5.6 17.3,-12.1 0,-6.2 -0.3,-40.4 -0.3,-61.4 0,0 -70,15 -84.7,-29.8 0,0 -11.4,-29.1 -27.8,-36.6 0,0 -22.9,-15.7 1.6,-15.4 0,0 24.9,2 38.6,25.8 21.9,38.6 58.6,27.5 72.9,20.9 2.3,-16 8.8,-27.1 16,-33.7 -55.9,-6.2 -112.3,-14.3 -112.3,-110.5 0,-27.5 7.6,-41.3 23.6,-58.9 -2.6,-6.5 -11.1,-33.3 2.6,-67.9 20.9,-6.5 69,27 69,27 20,-5.6 41.5,-8.5 62.8,-8.5 21.3,0 42.8,2.9 62.8,8.5 0,0 48.1,-33.6 69,-27 13.7,34.7 5.2,61.4 2.6,67.9 16,17.7 25.8,31.5 25.8,58.9 0,96.5 -58.9,104.2 -114.8,110.5 9.2,7.9 17,22.9 17,46.4 0,33.7 -0.3,75.4 -0.3,83.6 0,6.5 4.6,14.4 17.3,12.1 C 428.2,457.8 496,362.9 496,252 496,113.3 383.5,8 244.8,8 Z"/>
        </svg>
    
</a>
        

        <a id="mode_toggle" href="#" @click.prevent="handleClick" :title="mode">
    <template v-if="mode == 'light'">
        <svg width="100%" height="100%" viewBox="0 0 79 80" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" style="fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:2;"><g id="mode_light"><rect id="Bounds" x="0" y="-0" width="78.623" height="79.049" style="fill:none;"/><circle cx="39.311" cy="39.524" r="15.734" style="fill:#fff;"/><g id="beams"><g id="beam"><path id="beam1" serif:id="beam" d="M44.212,4.901c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,-0 -4.9,2.196 -4.9,4.901l-0,9.614c-0,2.705 2.196,4.901 4.9,4.901c2.705,0 4.901,-2.196 4.901,-4.901l0,-9.614Z" style="fill:#fff;"/></g><g id="beam2" serif:id="beam"><path id="beam3" serif:id="beam" d="M67.48,18.073c1.913,-1.912 1.913,-5.018 0,-6.931c-1.912,-1.912 -5.018,-1.912 -6.931,0l-6.798,6.799c-1.912,1.912 -1.912,5.018 0,6.931c1.913,1.912 5.018,1.912 6.931,-0l6.798,-6.799Z" style="fill:#fff;"/></g><g id="beam4" serif:id="beam"><path id="beam5" serif:id="beam" d="M25.728,61.108c1.912,-1.913 1.912,-5.018 -0,-6.931c-1.913,-1.913 -5.019,-1.913 -6.931,-0l-6.799,6.798c-1.912,1.913 -1.912,5.019 0,6.931c1.913,1.913 5.019,1.913 6.931,0l6.799,-6.798Z" style="fill:#fff;"/></g><g id="beam6" serif:id="beam"><path id="beam7" serif:id="beam" d="M60.682,54.177c-1.913,-1.913 -5.018,-1.913 -6.931,-0c-1.912,1.913 -1.912,5.018 0,6.931l6.798,6.798c1.913,1.913 5.019,1.913 6.931,0c1.913,-1.912 1.913,-5.018 0,-6.931l-6.798,-6.798Z" style="fill:#fff;"/></g><g id="beam8" serif:id="beam"><path id="beam9" serif:id="beam" d="M4.901,34.623c-2.705,0 -4.901,2.196 -4.901,4.901c0,2.705 2.196,4.901 4.901,4.901l9.614,0c2.705,0 4.901,-2.196 4.901,-4.901c0,-2.705 -2.196,-4.901 -4.901,-4.901l-9.614,0Z" style="fill:#fff;"/></g><g id="beam10" serif:id="beam"><path id="beam11" serif:id="beam" d="M44.212,64.534c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,-0 -4.9,2.196 -4.9,4.901l-0,9.614c-0,2.705 2.196,4.901 4.9,4.901c2.705,-0 4.901,-2.196 4.901,-4.901l0,-9.614Z" style="fill:#fff;"/></g><g id="beam12" serif:id="beam"><path id="beam13" serif:id="beam" d="M18.929,11.142c-1.912,-1.912 -5.018,-1.912 -6.931,0c-1.912,1.913 -1.912,5.019 0,6.931l6.799,6.799c1.912,1.912 5.018,1.912 6.931,-0c1.912,-1.913 1.912,-5.019 -0,-6.931l-6.799,-6.799Z" style="fill:#fff;"/></g><g id="beam14" serif:id="beam"><path id="beam15" serif:id="beam" d="M64.108,34.623c-2.705,0 -4.901,2.196 -4.901,4.901c-0,2.705 2.196,4.901 4.901,4.901l9.614,0c2.705,0 4.901,-2.196 4.901,-4.901c-0,-2.705 -2.196,-4.901 -4.901,-4.901l-9.614,0Z" style="fill:#fff;"/></g></g></g></svg>
    </template>

    <template v-if="mode == 'dark'">
        <svg width="100%" height="100%" viewBox="0 0 79 80" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" style="fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:2;"><g id="mode_dark"><rect id="Bounds" x="0" y="-0" width="78.623" height="79.049" style="fill:none;"/><circle cx="39.311" cy="39.524" r="15.734" style="fill:#fff;"/><g id="beams"><g id="beam"><path id="beam1" serif:id="beam" d="M44.212,14.515c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,0 -4.901,2.196 -4.901,4.901c0,2.705 2.197,4.901 4.901,4.901c2.705,0 4.901,-2.196 4.901,-4.901Z" style="fill:#fff;"/></g><g id="beam2" serif:id="beam"><path id="beam3" serif:id="beam" d="M60.662,24.892c1.902,-1.902 1.902,-4.99 0,-6.892l-0.04,-0.039c-1.901,-1.902 -4.989,-1.902 -6.891,-0c-1.901,1.901 -1.901,4.989 0,6.891l0.04,0.04c1.902,1.901 4.989,1.901 6.891,-0Z" style="fill:#fff;"/></g><g id="beam4" serif:id="beam"><path id="beam5" serif:id="beam" d="M25.732,61.103c1.91,-1.91 1.91,-5.011 0,-6.921l-0.009,-0.01c-1.91,-1.91 -5.012,-1.91 -6.921,-0c-1.91,1.91 -1.91,5.011 -0,6.921l0.01,0.01c1.909,1.91 5.011,1.91 6.92,-0Z" style="fill:#fff;"/></g><g id="beam6" serif:id="beam"><path id="beam7" serif:id="beam" d="M60.672,54.167c-1.907,-1.907 -5.004,-1.907 -6.911,0l-0.02,0.02c-1.907,1.907 -1.907,5.004 0,6.911c1.907,1.907 5.004,1.907 6.911,-0l0.02,-0.02c1.907,-1.907 1.907,-5.004 0,-6.911Z" style="fill:#fff;"/></g><g id="beam8" serif:id="beam"><path id="beam9" serif:id="beam" d="M14.52,34.623c-2.702,0 -4.896,2.194 -4.896,4.896l0,0.01c0,2.702 2.194,4.896 4.896,4.896c2.702,0 4.896,-2.194 4.896,-4.896l-0,-0.01c-0,-2.702 -2.194,-4.896 -4.896,-4.896Z" style="fill:#fff;"/></g><g id="beam10" serif:id="beam"><path id="beam11" serif:id="beam" d="M44.212,64.534c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,-0 -4.901,2.196 -4.901,4.901c0,2.704 2.197,4.9 4.901,4.9c2.705,0 4.901,-2.196 4.901,-4.9Z" style="fill:#fff;"/></g><g id="beam12" serif:id="beam"><path id="beam13" serif:id="beam" d="M25.73,17.943c-1.911,-1.911 -5.015,-1.911 -6.926,0l-0.005,0.005c-1.911,1.911 -1.911,5.015 0,6.926c1.911,1.911 5.015,1.911 6.926,0l0.005,-0.005c1.911,-1.911 1.911,-5.014 -0,-6.926Z" style="fill:#fff;"/></g><g id="beam14" serif:id="beam"><path id="beam15" serif:id="beam" d="M64.098,34.623c-2.699,0 -4.891,2.192 -4.891,4.892l-0,0.019c-0,2.699 2.192,4.891 4.891,4.891c2.7,0 4.892,-2.192 4.892,-4.891l0,-0.019c0,-2.7 -2.192,-4.892 -4.892,-4.892Z" style="fill:#fff;"/></g></g></g></svg>
    </template>

    <template v-if="mode == 'darkest'">
        <svg width="100%" height="100%" viewBox="0 0 79 80" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" style="fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:2;"><g id="mode_darkest"><rect id="Bounds" x="0" y="-0" width="78.623" height="79.049" style="fill:none;"/><path d="M39.315,23.791c8.684,-0 15.734,7.05 15.734,15.733c0,8.684 -7.05,15.734 -15.734,15.734c-8.683,0 -15.733,-7.05 -15.733,-15.734c-0,-8.683 7.05,-15.733 15.733,-15.733Zm0,4.737c6.069,0 10.997,4.927 10.997,10.996c-0,6.069 -4.928,10.996 -10.997,10.996c-6.068,0 -10.996,-4.927 -10.996,-10.996c0,-6.069 4.928,-10.996 10.996,-10.996Z" style="fill:#fff;"/><g id="beams"><g id="beam"><path id="beam1" serif:id="beam" d="M44.216,14.515c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,0 -4.9,2.196 -4.9,4.901c-0,2.705 2.196,4.901 4.9,4.901c2.705,0 4.901,-2.196 4.901,-4.901Z" style="fill:#fff;"/></g><g id="beam2" serif:id="beam"><path id="beam3" serif:id="beam" d="M60.666,24.892c1.902,-1.902 1.902,-4.99 0,-6.892l-0.04,-0.039c-1.901,-1.902 -4.989,-1.902 -6.891,-0c-1.901,1.901 -1.901,4.989 0,6.891l0.04,0.04c1.902,1.901 4.99,1.901 6.891,-0Z" style="fill:#fff;"/></g><g id="beam4" serif:id="beam"><path id="beam5" serif:id="beam" d="M25.737,61.103c1.909,-1.91 1.909,-5.011 -0,-6.921l-0.01,-0.01c-1.91,-1.91 -5.011,-1.91 -6.921,-0c-1.91,1.91 -1.91,5.011 -0,6.921l0.01,0.01c1.91,1.91 5.011,1.91 6.921,-0Z" style="fill:#fff;"/></g><g id="beam6" serif:id="beam"><path id="beam7" serif:id="beam" d="M60.676,54.167c-1.907,-1.907 -5.004,-1.907 -6.911,0l-0.02,0.02c-1.907,1.907 -1.907,5.004 0,6.911c1.907,1.907 5.004,1.907 6.911,-0l0.02,-0.02c1.907,-1.907 1.907,-5.004 0,-6.911Z" style="fill:#fff;"/></g><g id="beam8" serif:id="beam"><path id="beam9" serif:id="beam" d="M14.524,34.623c-2.702,0 -4.896,2.194 -4.896,4.896l0,0.01c0,2.702 2.194,4.896 4.896,4.896c2.702,0 4.896,-2.194 4.896,-4.896l0,-0.01c0,-2.702 -2.194,-4.896 -4.896,-4.896Z" style="fill:#fff;"/></g><g id="beam10" serif:id="beam"><path id="beam11" serif:id="beam" d="M44.216,64.534c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,-0 -4.9,2.196 -4.9,4.901c-0,2.704 2.196,4.9 4.9,4.9c2.705,0 4.901,-2.196 4.901,-4.9Z" style="fill:#fff;"/></g><g id="beam12" serif:id="beam"><path id="beam13" serif:id="beam" d="M25.734,17.943c-1.911,-1.911 -5.015,-1.911 -6.926,0l-0.005,0.005c-1.911,1.911 -1.911,5.015 0,6.926c1.911,1.911 5.015,1.911 6.926,0l0.005,-0.005c1.911,-1.911 1.911,-5.014 0,-6.926Z" style="fill:#fff;"/></g><g id="beam14" serif:id="beam"><path id="beam15" serif:id="beam" d="M64.103,34.623c-2.7,0 -4.892,2.192 -4.892,4.892l-0,0.019c-0,2.699 2.192,4.891 4.892,4.891c2.699,0 4.891,-2.192 4.891,-4.891l0,-0.019c0,-2.7 -2.192,-4.892 -4.891,-4.892Z" style="fill:#fff;"/></g></g></g></svg>
    </template>
</a>

<script>
(function() {
    const LOCAL_STORAGE_KEY = 'piccoloThemeMode'

    var initialMode = localStorage.getItem(LOCAL_STORAGE_KEY)

    if (initialMode) {
        // Make sure the value in local storage is valid
        if (['light', 'dark', 'darkest'].indexOf(initialMode) == -1) {
            initialMode = 'light'
            localStorage.setItem(LOCAL_STORAGE_KEY, initialMode)
        }
    } else {
        // Check if the client prefers dark mode
        if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
            initialMode = 'dark'
        } else {
            initialMode = 'light'
        }
        localStorage.setItem(LOCAL_STORAGE_KEY, initialMode)
    }

    document.documentElement.dataset.mode = initialMode

    PetiteVue.createApp({
        'mode': initialMode,
        handleClick() {
            let currentMode = this.mode

            if (currentMode == 'light') {
                this.mode = 'dark'
            } else if (currentMode == 'dark') {
                this.mode = 'darkest'
            } else if (currentMode == 'darkest') {
                this.mode = 'light'
            }

            document.documentElement.dataset.mode = this.mode
            localStorage.setItem(LOCAL_STORAGE_KEY, this.mode)

            console.log(this.mode)
        }
    }).mount('#mode_toggle')
})()
</script>
            <p class="mobile_search_link">
                <a href="../../search.html" title="Search">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 65 64" fill-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="2">
                        <path d="M14.873 40.009c-2.315-3.943-3.642-8.532-3.642-13.429C11.231 11.91 23.141 0 37.811 0s26.58 11.91 26.58 26.58-11.91 26.58-26.58 26.58a26.44 26.44 0 0 1-14.277-4.161L9.739 62.794a3.12 3.12 0 0 1-4.413 0L.913 58.382c-1.217-1.218-1.217-3.196 0-4.413l13.96-13.96zM37.811 8.054c10.225 0 18.526 8.301 18.526 18.526s-8.301 18.526-18.526 18.526-18.526-8.301-18.526-18.526S27.586 8.054 37.811 8.054z" fill="#fff" />
                    </svg>
                </a>
            </p>
        

        <div class="searchbox_wrapper">
            
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
    </nav>
</div>

    
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper"><ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Mathematics</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../ft/index.html">Fourier Transform</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">Statistical Learning</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="mse.html">Mean Squared Error Decomposition</a></li>
<li class="toctree-l3"><a class="reference internal" href="bayes.html">Bayes Theorem</a></li>
<li class="toctree-l3"><a class="reference internal" href="bayes_ex_1.html">Bayes Example: Biased Coin</a></li>
<li class="toctree-l3"><a class="reference internal" href="bayes_ex_2.html">Bayes Example: Classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="mgd.html">Multivariate Gaussian Distribution</a></li>
<li class="toctree-l3"><a class="reference internal" href="lda.html">LDA: Classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="lda_dr.html">LDA: Dimensionality Reduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="qda.html">Quadratic Discriminant Analysis</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Linear Regression: Ordinary Least Squares</a></li>
<li class="toctree-l3"><a class="reference internal" href="lr_formula.html">Linear Regression: Formula Derivation</a></li>
<li class="toctree-l3"><a class="reference internal" href="gd.html">Gradient Descent</a></li>
<li class="toctree-l3"><a class="reference internal" href="pg.html">Policy Gradient</a></li>
<li class="toctree-l3"><a class="reference internal" href="appx_a.html">Appendix A: Setup R Environment</a></li>
<li class="toctree-l3"><a class="reference internal" href="appx_b.html">Appendix B: Setup Jupyter Lab Environment</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../nlp/index.html">NLP</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cv/index.html">Computer Vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ref.html">Reference</a></li>
</ul>

        </div>
      </div>


    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="linear-regression-ordinary-least-squares">
<h1>Linear Regression: Ordinary Least Squares<a class="headerlink" href="#linear-regression-ordinary-least-squares" title="Permalink to this heading">¶</a></h1>
<section id="tl-dr">
<h2>TL;DR<a class="headerlink" href="#tl-dr" title="Permalink to this heading">¶</a></h2>
<ul class="simple">
<li><p>OLS estimator is unbiased if:</p>
<ul>
<li><p>No Collinearity</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathrm{E} \left[ \epsilon_i \right] = 0,\)</span>
<span class="math notranslate nohighlight">\(\forall i \in \left[ 1, n \right]\)</span></p></li>
</ul>
</li>
<li><p>OLS estimator is the Best Linear Unbiased Estimator (BLUE) if:</p>
<ul>
<li><p>No Collinearity</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathrm{E} \left[ \epsilon_i \right] = 0,\)</span>
<span class="math notranslate nohighlight">\(\forall i \in \left[ 1, n \right]\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathrm{Var} \left[ \epsilon_i \right] = \sigma^2,\)</span>
<span class="math notranslate nohighlight">\(\forall i \in \left[ 1, n \right]\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathrm{Cov} \left[ \epsilon_i, \epsilon_j \right] = 0,\)</span>
<span class="math notranslate nohighlight">\(\forall i \ne j\)</span></p></li>
</ul>
</li>
<li><p>OLS estimator is equivalent to MLE (generalized linear model) if:</p>
<ul>
<li><p>No Collinearity</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathrm{E} \left[ \epsilon_i \right] = 0,\)</span>
<span class="math notranslate nohighlight">\(\forall i \in \left[ 1, n \right]\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathrm{Var} \left[ \epsilon_i \right] = \sigma^2,\)</span>
<span class="math notranslate nohighlight">\(\forall i \in \left[ 1, n \right]\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathrm{Cov} \left[ \epsilon_i, \epsilon_j \right] = 0,\)</span>
<span class="math notranslate nohighlight">\(\forall i \ne j\)</span></p></li>
<li><p>random errors are <strong>identically</strong> and <strong>independently</strong> drawn from a
<strong>normal distribution</strong>.</p></li>
</ul>
</li>
</ul>
</section>
<section id="problem-and-hypothesis">
<h2>Problem and Hypothesis<a class="headerlink" href="#problem-and-hypothesis" title="Permalink to this heading">¶</a></h2>
<p>Suppose:</p>
<div class="math notranslate nohighlight">
\[\mathbf{y} = \mathbf{X} \mathbf{w}^* + \mathbf{\epsilon}\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is the design matrix of <span class="math notranslate nohighlight">\(n \times p\)</span> non-random, observable
predictors <a class="footnote-reference brackets" href="#f01" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{\epsilon}\)</span> is a <span class="math notranslate nohighlight">\(n\)</span>-dimensional random vector, which makes
<span class="math notranslate nohighlight">\(\mathbf{y}\)</span> the <span class="math notranslate nohighlight">\(n\)</span>-dimensional observable random vector response</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{w}^* = [w_1^*, \ldots, w_p^*]^T\)</span> are the <strong>underlying non-random,
unobservable coefficients</strong>, which is to be estimated.</p></li>
</ul>
<p>Our hypothesis is:</p>
<div class="math notranslate nohighlight">
\[\hat{\mathbf{y}} = \mathbf{X} \mathbf{w}\]</div>
<p>where the linear estimator <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> can be represented as:</p>
<div class="math notranslate nohighlight">
\[\mathbf{w} = \mathbf{C} \mathbf{y}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{C}\)</span> is a <span class="math notranslate nohighlight">\(p \times n\)</span> matrix depends on the <span class="math notranslate nohighlight">\(n \times p\)</span>
predictor <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> and the <span class="math notranslate nohighlight">\(n \times 1\)</span> response <span class="math notranslate nohighlight">\(\mathbf{y}\)</span>.</p>
<p>A natural thought would be that <span class="math notranslate nohighlight">\(\mathbf{c}\)</span> is the
<a class="reference external" href="https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse">pseudoinverse</a>
of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>, which will be confirmed by the OLS.</p>
</section>
<section id="assumptions-of-the-ols">
<h2>Assumptions of the OLS<a class="headerlink" href="#assumptions-of-the-ols" title="Permalink to this heading">¶</a></h2>
<section id="no-collinearity">
<h3>No Collinearity<a class="headerlink" href="#no-collinearity" title="Permalink to this heading">¶</a></h3>
<p>To get the assumption we need to calculate the analytical solution to the OLS
problem.</p>
<p>With Ordinary Least Squares (OLS), the risk can be calculated as:</p>
<div class="math notranslate nohighlight">
\[R (\hat{\mathbf{w}}) =
\frac{1}{n} \lVert \hat{\mathbf{y}} - \mathbf{y} \rVert_2^2\]</div>
<p>To minimize <span class="math notranslate nohighlight">\(R\)</span> <span id="id2">[<a class="reference internal" href="../../ref.html#id4" title="Ian J. Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, Cambridge, MA, USA, 2016. URL: http://www.deeplearningbook.org.">5</a>]</span> <span id="id3">[<a class="reference internal" href="../../ref.html#id6" title="K. B. Petersen and M. S. Pedersen. The matrix cookbook. nov 2012. Version 20121115. URL: http://www2.compute.dtu.dk/pubdb/pubs/3274-full.html.">9</a>]</span>:</p>
<div class="math notranslate nohighlight">
\[\nabla_{\mathbf{w}} R = 0\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\therefore
\nabla_{\mathbf{w}} R &amp;=
\nabla_{\mathbf{w}}
\frac{1}{n} \lVert \hat{\mathbf{y}} - \mathbf{y} \rVert_2^2
\\ &amp;=
\nabla_{\mathbf{w}}
  (\hat{\mathbf{y}} - \mathbf{y})^T
  (\hat{\mathbf{y}} - \mathbf{y})
\\ &amp;=
\nabla_{\mathbf{w}}
  (\mathbf{X} \mathbf{w} - \mathbf{y})^T
  (\mathbf{X} \mathbf{w} - \mathbf{y})
\\ &amp;=
\nabla_{\mathbf{w}}
  (\mathbf{w}^T \mathbf{X}^T - \mathbf{y}^T)
  (\mathbf{X} \mathbf{w} - \mathbf{y})
\\ &amp;=
\nabla_{\mathbf{w}}
  (\mathbf{w}^T \mathbf{X}^T \mathbf{X} \mathbf{w} -
   \mathbf{w}^T \mathbf{X}^T \mathbf{y} -
   \mathbf{y}^T \mathbf{X} \mathbf{w} +
  \mathbf{y}^T \mathbf{y})
\\ &amp;=
\nabla_{\mathbf{w}}
  (\mathbf{w}^T \mathbf{X}^T \mathbf{X} \mathbf{w} -
   2 \mathbf{w}^T \mathbf{X}^T \mathbf{y} -
   \mathbf{y}^T \mathbf{y})
\\ &amp;=
2 \mathbf{X}^T \mathbf{X} \mathbf{w} - 2 \mathbf{X}^T \mathbf{y}
\\ &amp;= 0\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\therefore
\mathbf{w}_{ols} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Why the <strong>no multi-collinearity</strong> assumption is needed:</p>
<ul>
<li><p>When matrix <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> has linearly independent columns, its
Moore–Penrose inverse <span id="id4">[<a class="reference internal" href="../../ref.html#id10" title="Wikipedia. Moore–Penrose Inverse. 2023. [Online; accessed 9-May-2023]. URL: https://en.wikipedia.org/wiki/Moore-Penrose_inverse.">18</a>]</span> <span class="math notranslate nohighlight">\(\mathbf{X}^+\)</span> can be computed as:</p>
<div class="math notranslate nohighlight">
\[\mathbf{X}^+ = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T\]</div>
</li>
<li><p>Obviously <span class="math notranslate nohighlight">\(\mathbf{X}^T \mathbf{X}\)</span> is a Gram matrix <span id="id5">[<a class="reference internal" href="../../ref.html#id11" title="Wikipedia. Gram Matrix. 2023. [Online; accessed 9-May-2023]. URL: https://en.wikipedia.org/wiki/Gram_matrix.">17</a>]</span>
over reals, which is symmetric and positive semi-definite, and it is
invertible if and only if its determinant is non-zero, which is equivalent
to the condition that <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> has linearly independent columns
<a class="footnote-reference brackets" href="#f02" id="id6" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>.</p></li>
</ul>
</div>
<p>An alternative form is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{w}_{ols} &amp;=
(\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T
(\mathbf{X} \mathbf{w}^* + \mathbf{\epsilon})
\\ &amp;=
\mathbf{w}^* + (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{\epsilon}\end{split}\]</div>
</section>
<section id="zero-mean-error-and-unbiased-estimator">
<h3>Zero Mean Error and Unbiased Estimator<a class="headerlink" href="#zero-mean-error-and-unbiased-estimator" title="Permalink to this heading">¶</a></h3>
<div class="math notranslate nohighlight">
\[\mathrm{E} \left[ \mathbf{\epsilon} \right] = \mathbf{0}
\implies
\mathrm{E} \left[ \mathbf{w}_{ols} \right] = \mathbf{w}^*\]</div>
<p>Proof:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathrm{E} \left[
  \mathbf{w}_{ols}
\right] &amp;=
\mathrm{E} \left[
  \mathbf{w}^* +
  (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{\epsilon}
\right]
\\ &amp;=
\mathbf{w}^* +
(\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T
\mathrm{E} \left[ \mathbf{\epsilon} \right]
\\ &amp;=
\mathbf{w}^* +
(\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{0}
\\ &amp;=
\mathbf{w}^*\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\tag*{$\blacksquare$}\]</div>
</section>
<section id="homoscedasticity-and-blue">
<h3>Homoscedasticity and BLUE<a class="headerlink" href="#homoscedasticity-and-blue" title="Permalink to this heading">¶</a></h3>
<p>The OLS estimator is the Best Linear Unbiased Estimator (BLUE) if:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\forall i \in \left[ 0, n \right] &amp;:
\mathrm{E} \left[ \mathbf{\epsilon} \right] = 0,
\mathrm{Var} \left[ \mathbf{\epsilon} \right] = \sigma^2\\\begin{split}\\\end{split}\\\forall i \ne j &amp;:
\mathrm{Cov} \left[ \mathbf{\epsilon} \right] = 0\end{aligned}\end{align} \]</div>
<p>Proof <span id="id7">[<a class="reference internal" href="../../ref.html#id14" title="Andrew Rothman. Ols regression, gauss-markov, blue, and understanding the math. Jun 2020. [Online; accessed 21-February-2023]. URL: https://towardsdatascience.com/ ols-linear-regression-gauss-markov-blue-and-understanding-the- math-453d7cc630a5.">14</a>]</span> <span id="id8">[<a class="reference internal" href="../../ref.html#id8" title="Wikipedia. Gauss–Markov Theorem. 2023. [Online; accessed 21-February-2023]. URL: https://en.wikipedia.org/wiki/Gauss–Markov_theorem.">16</a>]</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{\epsilon} \mathbf{\epsilon}^T &amp;=
\begin{bmatrix}
  \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_n
\end{bmatrix}
\cdot
\begin{bmatrix}
  \epsilon_1 &amp; \epsilon_2 &amp; \cdots &amp; \epsilon_n
\end{bmatrix}
\\ &amp;=
\begin{bmatrix}
  \epsilon_1 \epsilon_1 &amp; \epsilon_1 \epsilon_2 &amp; \cdots &amp;
    \epsilon_1 \epsilon_n
  \\
  \epsilon_2 \epsilon_1 &amp; \epsilon_2 \epsilon_2 &amp; \cdots &amp;
    \epsilon_2 \epsilon_n
  \\
  \vdots &amp; \vdots &amp; \ddots &amp; \vdots
  \\
  \epsilon_n \epsilon_1 &amp; \epsilon_n \epsilon_2 &amp; \cdots &amp;
    \epsilon_n \epsilon_n
\end{bmatrix}
\\ &amp;=
\begin{bmatrix}
  (\epsilon_1 - 0) (\epsilon_1 - 0) &amp; (\epsilon_1 - 0) (\epsilon_2 - 0)
    &amp; \cdots &amp; (\epsilon_1 - 0) (\epsilon_n - 0)
  \\
  (\epsilon_2 - 0) (\epsilon_1 - 0) &amp; (\epsilon_2 - 0) (\epsilon_2 - 0)
    &amp; \cdots &amp; (\epsilon_2 - 0) (\epsilon_n - 0)
  \\
  \vdots &amp; \vdots &amp; \ddots &amp; \vdots
  \\
  (\epsilon_n - 0) (\epsilon_1 - 0) &amp; (\epsilon_n - 0) (\epsilon_2 - 0)
    &amp; \cdots &amp; (\epsilon_n - 0) (\epsilon_n - 0)
\end{bmatrix}\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\therefore
\mathrm{E} \left[
  \mathbf{\epsilon} \mathbf{\epsilon}^T
\right] &amp;=
\begin{bmatrix}
  \mathrm{E} \left[
    (\epsilon_1 - 0) (\epsilon_1 - 0)
  \right] &amp;
  \mathrm{E} \left[
    (\epsilon_1 - 0) (\epsilon_2 - 0)
  \right] &amp;
  \cdots &amp;
  \mathrm{E} \left[
    (\epsilon_1 - 0) (\epsilon_n - 0)
  \right]
  \\
  \mathrm{E} \left[
    (\epsilon_2 - 0) (\epsilon_1 - 0)
  \right] &amp;
  \mathrm{E} \left[
    (\epsilon_2 - 0) (\epsilon_2 - 0)
  \right] &amp;
  \cdots &amp;
  \mathrm{E} \left[
    (\epsilon_2 - 0) (\epsilon_n - 0)
  \right]
  \\
  \vdots &amp; \vdots &amp; \ddots &amp; \vdots
  \\
  \mathrm{E} \left[
    (\epsilon_n - 0) (\epsilon_1 - 0)
  \right] &amp;
  \mathrm{E} \left[
    (\epsilon_n - 0) (\epsilon_2 - 0)
  \right] &amp;
  \cdots &amp;
  \mathrm{E} \left[
    (\epsilon_n - 0) (\epsilon_n - 0)
  \right]
\end{bmatrix}
\\ &amp;=
\begin{bmatrix}
  \mathrm{Var} \left[ \epsilon_1 \right] &amp;
  \mathrm{Cov} \left[ \epsilon_1, \epsilon_2 \right] &amp;
  \cdots &amp;
  \mathrm{Cov} \left[ \epsilon_1, \epsilon_n \right]
  \\
  \mathrm{Cov} \left[ \epsilon_2, \epsilon_1 \right] &amp;
  \mathrm{Var} \left[ \epsilon_2 \right] &amp;
  \cdots &amp;
  \mathrm{Cov} \left[ \epsilon_2, \epsilon_n \right]
  \\
  \vdots &amp; \vdots &amp; \ddots &amp; \vdots
  \\
  \mathrm{Cov} \left[ \epsilon_n, \epsilon_1 \right] &amp;
  \mathrm{Cov} \left[ \epsilon_n, \epsilon_2 \right] &amp;
  \cdots &amp;
  \mathrm{Var} \left[ \epsilon_n \right]
\end{bmatrix}
\\ &amp;=
\sigma^2 \mathbf{I}\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\therefore
\mathrm{Var} \left[ \mathbf{w}_{ols} \right] &amp;=
\mathrm{E} \left[
  (\mathbf{w}_{ols} - \mathrm{E} \left[
    \mathbf{w}_{ols}
  \right])
  (\mathbf{w}_{ols} - \mathrm{E} \left[
    \mathbf{w}_{ols}
  \right])^T
\right]
\\ &amp;=
\mathrm{E} \left[
  ((\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{\epsilon})
  ((\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{\epsilon})^T
\right]
\\ &amp;=
\mathrm{E} \left[
  (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{\epsilon}
  \mathbf{\epsilon}^T \mathbf{X} (\mathbf{X}^T \mathbf{X})^{-1}
\right]
\\ &amp;=
  (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T
  \mathrm{E} \left[
    \mathbf{\epsilon} \mathbf{\epsilon}^T
  \right]
  \mathbf{X} (\mathbf{X}^T \mathbf{X})^{-1}
\\ &amp;=
  \sigma^2 (\mathbf{X}^T \mathbf{X})^{-1}
  \mathbf{X}^T \mathbf{X} (\mathbf{X}^T \mathbf{X})^{-1}
\\ &amp;=
  \sigma^2 (\mathbf{X}^T \mathbf{X})^{-1}\end{split}\]</div>
<p>Let <span class="math notranslate nohighlight">\(\tilde{\mathbf{w}} = \tilde{\mathbf{C}} \mathbf{y}\)</span> be another <strong>unbiased
linear estimator</strong> of <span class="math notranslate nohighlight">\(\mathbf{w}^*\)</span> with
<span class="math notranslate nohighlight">\(\tilde{\mathbf{C}} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T + \mathbf{D}\)</span>
where <span class="math notranslate nohighlight">\(\mathbf{D}\)</span> is a <span class="math notranslate nohighlight">\(p \times n\)</span> non-zero matrix.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathrm{E} \left[ \tilde{\mathbf{w}} \right] &amp;=
\mathrm{E} \left[ \tilde{\mathbf{C}} \mathbf{y} \right]
\\ &amp;=
\mathrm{E} \left[
  ((\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T + \mathbf{D})
  (\mathbf{X} \mathbf{w}^* + \mathbf{\epsilon})
\right]
\\ &amp;=
((\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T + \mathbf{D})
\mathbf{X} \mathbf{w}^* +
((\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T + \mathbf{D})
\mathrm{E} \left[ \mathbf{\epsilon} \right]
\\ &amp;=
((\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T + \mathbf{D})
\mathbf{X} \mathbf{w}^*
\\ &amp;=
\mathbf{w}^* + \mathbf{D} \mathbf{X} \mathbf{w}^*\end{split}\]</div>
<p>Since <span class="math notranslate nohighlight">\(\tilde{\mathbf{w}}\)</span> is unbiased:</p>
<div class="math notranslate nohighlight">
\[\therefore
\mathbf{D} \mathbf{X} = 0\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\therefore
\mathrm{Var} \left[
  \tilde{\mathbf{w}}
\right] &amp;=
\mathrm{Var} \left[
  \tilde{\mathbf{C}} \mathbf{y}
\right]
\\ &amp;=
\tilde{\mathbf{C}} \mathrm{Var} \left[ \mathbf{y} \right]
\tilde{\mathbf{C}}^T
\\ &amp;=
\sigma^2 \tilde{\mathbf{C}} \tilde{\mathbf{C}}^T
\\ &amp;=
\sigma^2
((\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T + \mathbf{D})
((\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T + \mathbf{D})^T
\\ &amp;=
\sigma^2
((\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T + \mathbf{D})
(\mathbf{X} (\mathbf{X}^T \mathbf{X})^{-1} + \mathbf{D}^T)
\\ &amp;=
\sigma^2
((\mathbf{X}^T \mathbf{X})^{-1} +
 (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{D}^T +
 \mathbf{D} \mathbf{X} (\mathbf{X}^T \mathbf{X})^{-1} +
 \mathbf{D} \mathbf{D}^T)
\\ &amp;=
\sigma^2 (\mathbf{X}^T \mathbf{X})^{-1} +
\sigma^2 (\mathbf{X}^T \mathbf{X})^{-1} (\mathbf{D} \mathbf{X})^T +
\sigma^2 \mathbf{D} \mathbf{X} (\mathbf{X}^T \mathbf{X})^{-1} +
\sigma^2 \mathbf{D} \mathbf{D}^T\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\because
\mathbf{D} \mathbf{X} = 0\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\therefore
\mathrm{Var} \left[
  \tilde{\mathbf{w}}
\right] &amp;=
\sigma^2 (\mathbf{X}^T \mathbf{X})^{-1} +
\sigma^2 \mathbf{D} \mathbf{D}^T
\\ &amp;=
\mathrm{Var} \left[ \mathbf{w}_{ols} \right] +
\sigma^2 \mathbf{D} \mathbf{D}^T\end{split}\]</div>
<p>Since <span class="math notranslate nohighlight">\(\mathbf{D} \mathbf{D}^T\)</span> is positive semidefinite matrix:</p>
<div class="math notranslate nohighlight">
\[\because
\mathbf{D} \mathbf{D}^T \text{ is positive semidefinite}\]</div>
<div class="math notranslate nohighlight">
\[\therefore
\mathrm{Var} \left[ \tilde{\mathbf{w}} \right] &gt;
\mathrm{Var} \left[ \mathbf{w}_{ols} \right]\]</div>
<div class="math notranslate nohighlight">
\[\tag*{$\blacksquare$}\]</div>
</section>
<section id="normally-distributed-error-and-mle">
<h3>Normally Distributed Error and MLE<a class="headerlink" href="#normally-distributed-error-and-mle" title="Permalink to this heading">¶</a></h3>
<p>The OLS is mathematically equivalent to Maximum Likelihood Estimation
if the error term <span class="math notranslate nohighlight">\(\epsilon_1, \ldots, \epsilon_n\)</span> are identically and
independently distributed from a normal distribution of zero mean.</p>
<p>Proof:</p>
<div class="math notranslate nohighlight">
\[\because
\epsilon_i =
y_i - \hat{y}_i =
y_i - \mathbf{x}_i \mathbf{w}
\sim
N(\mu, 0)\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\therefore
\mathcal{L} (\mathbf{w} \mid \mathbf{X}) &amp;=
\prod_{i=1}^{n} \frac{1}{\sigma \sqrt{2 \pi}}
e^{-\frac{(y_i - \mathbf{x}_i \mathbf{w})^2}{2 \sigma^2}}
\\ &amp;=
(\frac{1}{\sigma \sqrt{2 \pi}})^n
\prod_{i=1}^{n}
e^{-\frac{(y_i - \mathbf{x}_i \mathbf{w})^2}{2 \sigma^2}}
\\ &amp;=
(2 \pi \sigma^2)^{-\frac{n}{2}}
\prod_{i=1}^{n}
e^{-\frac{(y_i - \mathbf{x}_i \mathbf{w})^2}{2 \sigma^2}}\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\therefore
\ln \mathcal{L} (\mathbf{w} \mid \mathbf{X}) &amp;=
-\frac{n}{2} \ln (2 \pi \sigma^2) +
\sum_{i=1}^n
-\frac{(y_i - \mathbf{x}_i \mathbf{w})^2}{2 \sigma^2}
\\ &amp;=
-\frac{n}{2} \ln (2 \pi \sigma^2) - \frac{1}{2 \sigma^2}
\sum_{i=1}^n
(y_i - \mathbf{x}_i \mathbf{w})^2
\\ &amp;=
-\frac{n}{2} \ln (2 \pi \sigma^2) - \frac{1}{2 \sigma^2}
(\mathbf{y} - \mathbf{X} \mathbf{w})^T (\mathbf{y} - \mathbf{X} \mathbf{w})\end{split}\]</div>
<p>To minimize <span class="math notranslate nohighlight">\(\ln \mathcal{L}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\nabla_{\mathbf{w}} \ln \mathcal{L} (\mathbf{w} \mid \mathbf{X}) = 0
\\
\implies
(\mathbf{y} - \mathbf{X} \mathbf{w})^T
(\mathbf{y} - \mathbf{X} \mathbf{w}) = 0\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\therefore
\mathbf{w}_{mle} =
(\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y} =
\mathbf{w}_{ols}\]</div>
<div class="math notranslate nohighlight">
\[\tag*{$\blacksquare$}\]</div>
<hr class="docutils" />
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="f01" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p><span class="math notranslate nohighlight">\(\mathbf{X}_{i1} = 1\)</span> for all <span class="math notranslate nohighlight">\(i \in [1, n]\)</span></p>
</aside>
<aside class="footnote brackets" id="f02" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">2</a><span class="fn-bracket">]</span></span>
<p>Particularly, if <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is centered, then
<span class="math notranslate nohighlight">\(\mathbf{X}^T \mathbf{X}\)</span> is the scatter matrix of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>,
proportional to the covariance matrix of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>.</p>
</aside>
</aside>
<p>Back to <a class="reference internal" href="index.html"><span class="doc">Statistical Learning</span></a>.</p>
<div id="disqus_thread" data-identifier="math/sl/lr_ols"></div></section>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
    
        <div id="show_right_sidebar">
            <p><a class="toggle_right_sidebar" href="#"><span class="icon">&lt;</span><span>Page contents<span></a></p>
        </div>

        <div id="right_sidebar">
            <p><a class="toggle_right_sidebar" href="#"><span class="icon">&gt;</span><span>Page contents:<span></a></p>
            <div class="page_toc">
                <ul>
<li><a class="reference internal" href="#">Linear Regression: Ordinary Least Squares</a><ul>
<li><a class="reference internal" href="#tl-dr">TL;DR</a></li>
<li><a class="reference internal" href="#problem-and-hypothesis">Problem and Hypothesis</a></li>
<li><a class="reference internal" href="#assumptions-of-the-ols">Assumptions of the OLS</a><ul>
<li><a class="reference internal" href="#no-collinearity">No Collinearity</a></li>
<li><a class="reference internal" href="#zero-mean-error-and-unbiased-estimator">Zero Mean Error and Unbiased Estimator</a></li>
<li><a class="reference internal" href="#homoscedasticity-and-blue">Homoscedasticity and BLUE</a></li>
<li><a class="reference internal" href="#normally-distributed-error-and-mle">Normally Distributed Error and MLE</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
        </div>
    

      <div class="clearer"></div>
    </div>
    <div class="button_nav_wrapper">
        <div class="button_nav">
            <div class="left">
                
                <a href="qda.html">
                    <span class="icon">&lt;</span><span>Quadratic Discriminant Analysis</span></a>
                
            </div>

            <div class="right">
                
                    <a href="lr_formula.html"><span>Linear Regression: Formula Derivation</span><span class="icon">&gt;</span></a>
                
            </div>
        </div>
    </div>


    <div class="footer" role="contentinfo">
        &#169; Copyright 2022, ZHANG, Meng.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.0.1.
    </div>

<p id="theme_credit">Styled using the <a href="https://github.com/piccolo-orm/piccolo_theme">Piccolo Theme</a></p>
  </body>
</html>