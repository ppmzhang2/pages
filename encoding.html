
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Input Encoding &#8212; GPT3-Study  documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/classic.css" />
    
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Neural Networks" href="network.html" />
    <link rel="prev" title="Model Design" href="model.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="network.html" title="Neural Networks"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="model.html" title="Model Design"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">GPT3-Study  documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Input Encoding</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="input-encoding">
<h1>Input Encoding<a class="headerlink" href="#input-encoding" title="Permalink to this headline">¶</a></h1>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>GPT-3 Input Encoding

                                &lt;---------- 50257 ----------&gt;

                                ============= 1 =============
                                ============= 2 =============
                                ============ ... ============
                                ============ 2048 ===========

                               |||                         |||
                               |||                         |||
                              \|||/                       \|||/
                               \|/                         \|/

                         token embedding           positional encoding
                              (WTE)                       (WPE)

                         &lt;--- 12288 ---&gt;             &lt;--- 12288 ---&gt;

                         ====== 1 ======             ====== 1 ======
                         ====== 2 ======             ====== 2 ======
                         ===== ... =====             ===== ... =====
                         ===== 2048 ====             ===== 2048 ====

                                |                           |
                                |                           |
                                -----------------------------
                                             |||
                                             |||
                                            \|||/
                                             \|/

                                           plus (+)

                                       &lt;--- 12288 ---&gt;

                                       ====== 1 ======
                                       ====== 2 ======
                                       ===== ... =====
                                       ===== 2048 ====
</pre></div>
</div>
<p>The input text will go through an embedding / encoding process, which will map
the string into a matrix:</p>
<ul>
<li><p>input text string</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="s2">&quot;python primer plus&quot;</span>
</pre></div>
</div>
</li>
<li><p>tokenized strings</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="s2">&quot;python&quot;</span><span class="p">,</span> <span class="s2">&quot;prim&quot;</span><span class="p">,</span> <span class="s2">&quot;er&quot;</span><span class="p">,</span> <span class="s2">&quot;plus&quot;</span><span class="p">]</span>
</pre></div>
</div>
</li>
<li><p>each token will be mapped to a token ID, ranging from <strong>0</strong> to <strong>50256</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">101</span><span class="p">]</span>
</pre></div>
</div>
</li>
<li><p>each token ID will be mapped to a 12288-dimension vector (GPT-3)</p></li>
</ul>
<p>Once tokenized, each of the input tokens will have two groups of properties:</p>
<ul class="simple">
<li><p>semantic properties as a sub-word, indicated by its token index ranging from
<strong>0</strong> to <strong>50256</strong></p></li>
<li><p>positional properties as an element in a sequence, indicated by its
positional index ranging from <strong>0</strong> to <strong>2047</strong></p></li>
</ul>
<p>The semantic properties will be interpreted by a “dictionary” in the form of a
matrix, and</p>
<section id="token-embedding-wte">
<h2>Token Embedding (WTE)<a class="headerlink" href="#token-embedding-wte" title="Permalink to this headline">¶</a></h2>
<p>To interpret a word we could use a dictionary and in the case of the GPT-3
training the “dictionary” is actually a <strong>50257-by-12288</strong> matrix, each row of
which is a <strong>12288-dimension</strong> vector, containing distance information with
other tokens</p>
<p>The GPT-3 has 2048 input slots and after tokenisation there the input will be
a <strong>2048-by-50257</strong> matrix. By multiplying the input matrix with the vocabulary
matrix mentioned above we have a <strong>2048-by-12288</strong> embedded matrix.</p>
<section id="questions">
<h3>Questions<a class="headerlink" href="#questions" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>does the token entries obey the additive law?</p></li>
</ul>
</section>
</section>
<section id="positional-encoding-wpe">
<h2>Positional Encoding (WPE)<a class="headerlink" href="#positional-encoding-wpe" title="Permalink to this headline">¶</a></h2>
<p>For each token in the 2048 input slots, its position is encoded passing the
index (from 0 to 2047) to <strong>12288</strong> sinusoidal functions of different frequencies.
The output is a <strong>2048-by-12288</strong> position encoded matrix.</p>
</section>
<section id="encoding-combination">
<h2>Encoding Combination<a class="headerlink" href="#encoding-combination" title="Permalink to this headline">¶</a></h2>
<p>Both the embedding matrix and position encoded matrix are <strong>2048-by-12288</strong>
matrices, and the final input to the model will be the sum of them.</p>
<p>Back to <a class="reference internal" href="index.html"><span class="doc">GPT3-Study’s Notes</span></a>.</p>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <div>
    <h3><a href="index.html">Table of Contents</a></h3>
    <ul>
<li><a class="reference internal" href="#">Input Encoding</a><ul>
<li><a class="reference internal" href="#token-embedding-wte">Token Embedding (WTE)</a><ul>
<li><a class="reference internal" href="#questions">Questions</a></li>
</ul>
</li>
<li><a class="reference internal" href="#positional-encoding-wpe">Positional Encoding (WPE)</a></li>
<li><a class="reference internal" href="#encoding-combination">Encoding Combination</a></li>
</ul>
</li>
</ul>

  </div>
  <div>
    <h4>Previous topic</h4>
    <p class="topless"><a href="model.html"
                          title="previous chapter">Model Design</a></p>
  </div>
  <div>
    <h4>Next topic</h4>
    <p class="topless"><a href="network.html"
                          title="next chapter">Neural Networks</a></p>
  </div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/encoding.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="network.html" title="Neural Networks"
             >next</a> |</li>
        <li class="right" >
          <a href="model.html" title="Model Design"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">GPT3-Study  documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Input Encoding</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2022, ZHANG, Meng.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 4.5.0.
    </div>
  </body>
</html>